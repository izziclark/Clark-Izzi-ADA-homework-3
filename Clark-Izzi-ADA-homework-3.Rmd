---
title: "Clark-Izzi-ADA-homework-3"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(curl)
library(broom)
library(tidyverse)
library(dplyr)
library(ggplot2)
f <- "https://raw.githubusercontent.com/difiore/ADA-datasets/master/KamilarAndCooperData.csv"
d <- read_csv(f, col_names = TRUE) # creates a "tibble"
```

# CHALLENGE 1

## Untransformed:
```{r}
m <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d)
m.summary <- tidy(m)
# pull coefficients
beta0 <- m.summary %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)
beta1 <- m.summary %>%
  filter(term == "Brain_Size_Species_Mean") %>%
  pull(estimate)
# 90% CI for slope parameter
CI <- confint(m, level = 0.9)
print(paste0("90% CI for slope: ", CI[[2]], ", ", CI[[4]]))
# confidence intervals using predict()
ci <- predict(m,
  newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean),
  interval = "confidence", level = 0.9
)
ci <- data.frame(ci)
ci <- cbind(d$Brain_Size_Species_Mean, ci)
names(ci) <- c("brainsize", "c.fit", "c.lwr", "c.upr")
# prediction intervals using predict()
pi <- predict(m,
  newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean),
  interval = "prediction", level = 0.9
)
pi <- data.frame(pi)
pi <- cbind(d$Brain_Size_Species_Mean, pi)
names(pi) <- c("brainsize", "p.fit", "p.lwr", "p.upr")
# scatterplot fitted with fitted line, 90% confidence and prediction interval bands, legend
p <- ggplot(data = d, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m)) +
  geom_point() +
  geom_text(x = 250, y = 1000, label = paste0("Longevity = ", beta0, " + ", beta1, " * Brain Size\n", "HA : β1 ≠ 0")) +
  geom_line(data = ci, aes(x = brainsize, y = c.fit, color = "black")) +
  # plot confidence interval lines
  geom_line(data = ci, aes(x = brainsize, y = c.lwr, color = "blue")) +
  geom_line(data = ci, aes(x = brainsize, y = c.upr, color = "blue")) +
  # plot prediction interval lines
  geom_line(data = pi, aes(x = brainsize, y = p.lwr, color = "red")) +
  geom_line(data = pi, aes(x = brainsize, y = p.upr, color = "red")) +
  scale_color_hue(name = "Lines", labels = c("Fit", "CI", "PI"))
p
```
  
### Point estimate of slope: 1.21799  
### 90% CI for slope: 0.886591372875068, 1.17709341399706  
### So we can reject the null hypothesis. As brain size increases by 1gm, longevity increases by 1.21799 months.  

## Log-log transformed:
```{r}
m_log <- lm(log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean), data = d)
m_log.summary <- tidy(m_log)
# pull coefficients
beta0_log <- m_log.summary %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)
beta1_log <- m_log.summary %>%
  filter(term == "log(Brain_Size_Species_Mean)") %>%
  pull(estimate)
# 90% CI for slope parameter
CI_log <- confint(m_log, level = 0.9)
print(paste0("90% CI for slope of log model: ", CI_log[[2]], ", ", CI_log[[4]]))
# confidence intervals using predict()
log_BrainSize <- log(d$Brain_Size_Species_Mean)
ci <- predict(m_log,
  newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean),
  interval = "confidence", level = 0.9
)
ci <- data.frame(ci)
ci <- cbind(log_BrainSize, ci)
names(ci) <- c("brainsize", "c.fit", "c.lwr", "c.upr")
# prediction intervals using predict()
pi <- predict(m_log,
  newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean),
  interval = "prediction", level = 0.9
)
pi <- data.frame(pi)
pi <- cbind(log_BrainSize, pi)
names(pi) <- c("brainsize", "p.fit", "p.lwr", "p.upr")
# plot
p_log <- ggplot(data = d, aes(x = log(Brain_Size_Species_Mean), y = log(MaxLongevity_m))) +
  geom_point() +
  geom_text(x = 3, y = 6.6, label = paste0("log(Longevity) = ", beta0_log, " + ", beta1_log, " * log(Brain Size)\n", "HA : β1 ≠ 0")) +
  geom_line(data = ci, aes(x = brainsize, y = c.fit, color = "black")) +
  # plot confidence interval lines
  geom_line(data = ci, aes(x = brainsize, y = c.lwr, color = "blue")) +
  geom_line(data = ci, aes(x = brainsize, y = c.upr, color = "blue")) +
  # plot prediction interval lines
  geom_line(data = pi, aes(x = brainsize, y = p.lwr, color = "red")) +
  geom_line(data = pi, aes(x = brainsize, y = p.upr, color = "red")) +
  scale_color_hue(name = "Lines", labels = c("Fit", "CI", "PI"))
p_log
```
  
### Point estimate of slope: 0.2341496  
### 90% CI for slope: 0.204639620689836, 0.26365953834772  
### So we can reject the null hypothesis. As brain size increases 2.71828-fold (natural log), longevity increases 1.263834-fold (exp(.2341496)).  

### Predict longevity for brain weight of 750gm
```{r}
# UNTRANSFORMED
long_750 <- beta0 + beta1 * 750
long_750
PI_750 <- predict(m,
  newdata = data.frame(Brain_Size_Species_Mean = 750),
  interval = "prediction", level = 0.90
)
PI_750

# LOG-LOG TRANSFORMED
long_log750 <- beta0_log + beta1_log * log(750)
long_log750
PI_log750 <- predict(m_log,
  newdata = data.frame(Brain_Size_Species_Mean = log(750)),
  interval = "prediction", level = 0.90
)
PI_log750
```
We can't trust this model to predict observations accurately for this value because it falls outside of the range of values for the explanatory variable in the sample.

### Which model is better?
A log-log transformed model is more appropriate here than untransformed, as brain size distribution is skewed toward the lower end. Transforming makes the close correlation between brain size and longevity more clear.

# CHALLENGE 2
1) Run linear regression of home range in relation to female mean body mass, report beta coefficients
```{r}
mod <- lm(log(HomeRange_km2) ~ log(Body_mass_female_mean), data = d)
mod.summary <- tidy(mod)
beta0 <- mod.summary %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)
beta1 <- mod.summary %>%
  filter(term == "log(Body_mass_female_mean)") %>%
  pull(estimate)
mod.summary
print(paste0("Beta0 = ", beta0))
print(paste0("Beta1 = ", beta1))
```
2) Bootstrap from the dataset 1000 times with replacement.
```{r}
boot <- data.frame(beta0 = 1:1000, beta1 = 1:1000) # dataframe to store bootstrap values
n <- nrow(d) # samples will be the same size as sample data
d_select <- select(d, Body_mass_female_mean, HomeRange_km2)
# loop through to sample 1000 times from original sample, generating linear model and pulling coefficients for each sample
for (i in 1:1000) {
  s <- sample_n(d_select, size = n, replace = TRUE)
  mod <- lm(log(HomeRange_km2) ~ log(Body_mass_female_mean), data = s)
  mod.summary <- tidy(mod)
  beta0 <- mod.summary %>%
    filter(term == "(Intercept)") %>%
    pull(estimate)
  beta1 <- mod.summary %>%
    filter(term == "log(Body_mass_female_mean)") %>%
    pull(estimate)
  # update dataframe storing coefficient values
  boot$beta0[[i]] <- beta0
  boot$beta1[[i]] <- beta1
}
# Plot histograms of sampling distributions for beta coefficients
hist(boot$beta0,
  breaks = 20, xlab = "beta0",
  main = "Bootstrapped Sampling Distribution: Beta0"
)
hist(boot$beta1,
  breaks = 20, xlab = "beta1",
  main = "Bootstrapped Sampling Distribution: Beta1"
)
```
  
3) Estimate standard error for each coefficients as standard deviation of sampling distribution from your bootstrap.
```{r}
SE_beta0 <- sd(boot$beta0)
SE_beta1 <- sd(boot$beta1)
print(paste0("Beta0 SE: ", SE_beta0))
print(paste0("Beta1 SE: ", SE_beta1))
```
4) Determine 95% CI for each coefficient based on appropriate quantiles from sampling distribution.
```{r}
alpha <- 0.05
# Beta0
beta0_lower <- quantile(boot$beta0, alpha / 2)
beta0_upper <- quantile(boot$beta0, 1 - (alpha / 2))
print(paste0("Beta0 CI: ", beta0_lower, ", ", beta0_upper))
# Beta1
beta1_lower <- quantile(boot$beta1, alpha / 2)
beta1_upper <- quantile(boot$beta1, 1 - (alpha / 2))
print(paste0("Beta1 CI: ", beta1_lower, ", ", beta1_upper))
```
5) How do the SEs estimated from the bootstrap sampling distribution compare to those estimated mathematically as part of lm() function? Similar!
```{r}
std.error_beta0 <- mod.summary %>%
    filter(term == "(Intercept)") %>%
    pull(std.error)
std.error_beta1 <- mod.summary %>%
    filter(term == "log(Body_mass_female_mean)") %>%
    pull(std.error)
SE_table <- data.frame(Coefficient = c("Beta0", "Beta1"), LinearModelSE = c(std.error_beta0, std.error_beta1), BootstrapSE = c(SE_beta0, SE_beta1))
SE_table
```
6) How do your bootstrap CIs compare to those estimated mathematically as part of the lm() function? Similar!
```{r}
CI <- confint(mod, level = 1 - alpha)
CI_table <- data.frame(Coefficient = c("Beta0", "Beta1"), LinearModelCIL = c(CI[[1]], CI[[2]]), LinearModelCIU = c(CI[[3]], CI[[4]]), BootstrapCIL = c(beta0_lower, beta1_lower), BootstrapCIU = c(beta0_upper, beta1_upper))
CI_table
```

# CHALLENGE 3
Create function boot_lm(), which returns beta coefficient names, values, standard errors, upper and lower CI limits for linear model based on original dataset, and mean beta coefficient estimates, SEs, and CI limits for those coefficients based on bootstrap.
```{r}
boot_lm <- function(d, model, conf.level = 0.95, reps = 1000){
  # Initialize dataframe for output
  df <- data.frame(Coefficent = c("Beta0", "Beta1"), Value=c(0,0), SE=c(0,0), CI_lower=c(0,0), CI_upper=c(0,0), meanBeta_boot=c(0,0), SE_boot=c(0,0), CI_lower_boot=c(0,0), CI_upper_boot=c(0,0))
  # Run lm on dataframe d using "model"
  m = lm(eval(parse(text = model)), data = d)
  m.summary <- tidy(m)
  # Fill dataframe values for linear model based on original dataset
  df$Value[1] <- m.summary %>%
    filter(term == term[[1]]) %>%
    pull(estimate)
  df$Value[2] <- m.summary %>%
    filter(term == term[[2]]) %>%
    pull(estimate)
  df$SE[1] <- m.summary %>%
    filter(term == term[[1]]) %>%
    pull(std.error)
  df$SE[2] <- m.summary %>%
    filter(term == term[[2]]) %>%
    pull(std.error)
  CI <- confint(m, level = conf.level) # confint creates table of lower and upper CI limits for beta0 and beta1
  df$CI_lower[1] <- CI[[1]]
  df$CI_lower[2] <- CI[[2]]
  df$CI_upper[1] <- CI[[3]]
  df$CI_upper[2] <- CI[[4]]
  # Bootstrap
  boot <- data.frame(beta0 = 1:reps, beta1 = 1:reps) # dataframe to store bootstrap values
  n <- nrow(d) # samples will be the same size as sample data
  for (i in 1:reps) {
    s <- sample_n(d, size = n, replace = TRUE)
    mod = lm(eval(parse(text = model)), data = s)
    mod.summary <- tidy(mod)
    beta0 <- mod.summary %>%
      filter(term == term[[1]]) %>%
      pull(estimate)
    beta1 <- mod.summary %>%
      filter(term == term[[2]]) %>%
      pull(estimate)
    # update dataframe storing coefficient values
    boot$beta0[[i]] <- beta0
    boot$beta1[[i]] <- beta1
  }
  # Fill dataframe values based on bootstrap
  df$meanBeta_boot[1] <- mean(boot$beta0)
  df$meanBeta_boot[2] <- mean(boot$beta1)
  df$SE_boot[1] <- sd(boot$beta0)
  df$SE_boot[2] <- sd(boot$beta1)
  ## confidence intervals using quantiles
  alpha <- 1 - conf.level
  # Beta0
  df$CI_lower_boot[1] <- quantile(boot$beta0, alpha / 2)
  df$CI_upper_boot[1] <- quantile(boot$beta0, 1 - (alpha / 2))
  # Beta1
  df$CI_lower_boot[2] <- quantile(boot$beta1, alpha / 2)
  df$CI_upper_boot[2] <- quantile(boot$beta1, 1 - (alpha / 2))
  return(df)
}
```
Run following models on KamilarAndCooper:
log(HomeRange_km2) ~ log(Body_mass_female_mean)
log(DayLength_km) ~ log(Body_mass_female_mean)
log(HomeRange_km2) ~ log(Body_mass_female_mean) + MeanGroupSize
```{r}
# run models through boot_lm()
boot_lm(d = d, model = "log(HomeRange_km2) ~ log(Body_mass_female_mean)")
boot_lm(d = d, model = "log(DayLength_km) ~ log(Body_mass_female_mean)")
boot_lm(d = d, model= "log(HomeRange_km2) ~ log(Body_mass_female_mean) + MeanGroupSize")
```

